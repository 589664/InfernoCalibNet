{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from src.XRayDataset import XRayDataset\n",
    "from src.utils import compute_mean_std, compute_class_weights, weighted_binary_crossentropy\n",
    "from src.prePro import preprocess_metadata, calculate_balanced_label_statistics, stratified_split_by_individual_labels\n",
    "from src.ICNTrainer import ICNTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "data_dir = os.getenv('DATA_DIR')\n",
    "\n",
    "filtered_df = preprocess_metadata(\n",
    "    f'../{data_dir}/raw/xraysMD.csv',\n",
    "    f'../{data_dir}/raw/xrays',\n",
    "    f'../{data_dir}/processed/xraysMD.csv'\n",
    ")\n",
    "\n",
    "filtered_df_stats = calculate_balanced_label_statistics(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7000\n",
      "Test/Validation set size: 3000\n",
      "\n",
      "Combined label distribution statistics:\n",
      "                    Training  Test/Validation\n",
      "Atelectasis              674              284\n",
      "Cardiomegaly             187              113\n",
      "Consolidation            241              107\n",
      "Edema                    100               40\n",
      "Effusion                 652              295\n",
      "Emphysema                138               59\n",
      "Fibrosis                 142               65\n",
      "Hernia                    17                6\n",
      "Infiltration            1030              451\n",
      "Mass                     294               97\n",
      "No Finding              4092             1744\n",
      "Nodule                   384              139\n",
      "Pleural_Thickening       198               75\n",
      "Pneumonia                 81               32\n",
      "Pneumothorax             237              118\n",
      "Total                   8467             3625\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = stratified_split_by_individual_labels(filtered_df, train_size=7000, test_size=3000)\n",
    "\n",
    "# Print sizes of the resulting DataFrames\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test/Validation set size: {len(test_df)}\")\n",
    "\n",
    "# Calculate the label distribution for the training and test sets\n",
    "train_distribution = pd.Series([label for labels in train_df['Labels'] for label in labels]).value_counts()\n",
    "test_distribution = pd.Series([label for labels in test_df['Labels'] for label in labels]).value_counts()\n",
    "\n",
    "# Combine both distributions into a DataFrame\n",
    "statistics_df = pd.DataFrame({\n",
    "    'Training': train_distribution,\n",
    "    'Test/Validation': test_distribution\n",
    "})\n",
    "\n",
    "# Fill NaN values with 0 (in case a label is not present in either set)\n",
    "statistics_df.fillna(0, inplace=True)\n",
    "\n",
    "# Add a total row to both columns\n",
    "statistics_df.loc['Total'] = statistics_df.sum()\n",
    "\n",
    "print(\"\\nCombined label distribution statistics:\")\n",
    "print(statistics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Mean: 139.45093172971428, Computed Std: 61.9332860849686\n"
     ]
    }
   ],
   "source": [
    "# Compute mean and std using a list of image paths\n",
    "image_paths = [f\"../data/raw/xrays/{image_id}.png\" for image_id in train_df['ImageID']]\n",
    "img_size = 1000  # Set to your desired size\n",
    "\n",
    "mean, std = compute_mean_std(image_paths, img_size)\n",
    "\n",
    "print(f\"Computed Mean: {mean}, Computed Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 139.45\n",
    "std = 61.93\n",
    "img_size = 500\n",
    "# Create training and validation datasets using the computed mean and std\n",
    "train_dataset = XRayDataset(dataframe=train_df, image_dir='../data/raw/xrays/', img_size=img_size, mean=mean, std=std)\n",
    "val_dataset = XRayDataset(dataframe=test_df, image_dir='../data/raw/xrays/', img_size=img_size, mean=mean, std=std)\n",
    "\n",
    "# Create DataLoaders for batching and shuffling\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: torch.Size([5, 1, 500, 500])\n",
      "Labels batch shape: torch.Size([5, 15])\n",
      "First image shape: torch.Size([1, 500, 500])\n",
      "First label: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Testing the DataLoader\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Print the shapes of the batch\n",
    "print(f\"Images batch shape: {images.shape}\")  # Should be [batch_size, channels, height, width]\n",
    "print(f\"Labels batch shape: {labels.shape}\")  # Should be [batch_size, num_labels]\n",
    "\n",
    "# Check the individual data points (optional)\n",
    "print(f\"First image shape: {images[0].shape}\")\n",
    "print(f\"First label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 1.618818998336792\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Move y_true and y_pred to the same device as class_weights (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = compute_class_weights(train_df, 15)\n",
    "# Simulate a forward pass with some random inputs and test the loss function\n",
    "y_true = torch.rand(5, 15).to(device)  # Same shape as your labels\n",
    "y_pred = torch.rand(5, 15).to(device)  # Same shape as your model output\n",
    "criterion = weighted_binary_crossentropy(class_weights)\n",
    "# Now compute the loss value\n",
    "loss_value = criterion(y_pred, y_true)\n",
    "print(f\"Loss value: {loss_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maksi\\Desktop\\master\\InfernoCalibNet\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\maksi\\Desktop\\master\\InfernoCalibNet\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nb3biznz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-moon-19</strong> at: <a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net/runs/nb3biznz' target=\"_blank\">https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net/runs/nb3biznz</a><br/> View project at: <a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net' target=\"_blank\">https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241024_031207-nb3biznz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nb3biznz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\maksi\\Desktop\\master\\InfernoCalibNet\\notebooks\\wandb\\run-20241024_031551-cpx8294g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net/runs/cpx8294g' target=\"_blank\">decent-grass-20</a></strong> to <a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net' target=\"_blank\">https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net/runs/cpx8294g' target=\"_blank\">https://wandb.ai/589664-h-gskulen-p-vestlandet/inferno-calib-net/runs/cpx8294g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/1400 [00:28<11:08:54, 28.69s/it, loss=0.888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/1400 [00:35<6:03:20, 15.59s/it, loss=0.419] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/1400 [00:39<4:07:17, 10.62s/it, loss=-0.042]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4/1400 [00:56<5:01:57, 12.98s/it, loss=-0.589]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5/1400 [01:14<5:45:52, 14.88s/it, loss=-1.08] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6/1400 [01:17<4:11:43, 10.83s/it, loss=-1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7/1400 [01:20<3:11:47,  8.26s/it, loss=-2.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 8/1400 [01:24<2:39:11,  6.86s/it, loss=-3.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 9/1400 [01:28<2:17:35,  5.93s/it, loss=-3.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 10/1400 [01:32<2:05:07,  5.40s/it, loss=-4.52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 11/1400 [01:36<1:55:39,  5.00s/it, loss=-5.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 12/1400 [02:00<4:10:27, 10.83s/it, loss=-6.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 13/1400 [02:05<3:29:37,  9.07s/it, loss=-6.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 14/1400 [02:25<4:42:27, 12.23s/it, loss=-7.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 15/1400 [02:31<4:00:58, 10.44s/it, loss=-8.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 16/1400 [02:39<3:40:06,  9.54s/it, loss=-9.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 17/1400 [02:46<3:25:56,  8.93s/it, loss=-9.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 18/1400 [02:53<3:13:39,  8.41s/it, loss=-10.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 19/1400 [02:58<2:46:36,  7.24s/it, loss=-11.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 20/1400 [03:02<2:24:11,  6.27s/it, loss=-12.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 21/1400 [03:07<2:17:21,  5.98s/it, loss=-13]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 22/1400 [03:11<2:01:30,  5.29s/it, loss=-13.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 23/1400 [03:15<1:51:28,  4.86s/it, loss=-14.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 24/1400 [03:19<1:46:19,  4.64s/it, loss=-15.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 25/1400 [03:23<1:45:22,  4.60s/it, loss=-16.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 26/1400 [03:27<1:41:00,  4.41s/it, loss=-17.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 27/1400 [03:31<1:34:47,  4.14s/it, loss=-18.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 28/1400 [03:36<1:41:28,  4.44s/it, loss=-18.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 29/1400 [03:40<1:40:13,  4.39s/it, loss=-19.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 30/1400 [04:13<4:57:18, 13.02s/it, loss=-20.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 31/1400 [04:27<5:03:00, 13.28s/it, loss=-22.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 32/1400 [04:42<5:15:12, 13.82s/it, loss=-23.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 33/1400 [04:57<5:20:33, 14.07s/it, loss=-24.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 34/1400 [05:11<5:23:14, 14.20s/it, loss=-25.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▎         | 35/1400 [05:25<5:18:54, 14.02s/it, loss=-29.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 36/1400 [05:46<6:03:38, 16.00s/it, loss=-29.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 37/1400 [05:47<4:25:46, 11.70s/it, loss=-30.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 38/1400 [05:52<3:40:45,  9.73s/it, loss=-31.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 39/1400 [05:55<2:54:25,  7.69s/it, loss=-32.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 40/1400 [05:58<2:21:35,  6.25s/it, loss=-32.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 41/1400 [06:02<2:03:20,  5.45s/it, loss=-33.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 42/1400 [06:05<1:48:50,  4.81s/it, loss=-33.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 43/1400 [06:08<1:37:59,  4.33s/it, loss=-34.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 44/1400 [06:12<1:30:46,  4.02s/it, loss=-35.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 45/1400 [06:15<1:25:58,  3.81s/it, loss=-35.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 46/1400 [06:18<1:21:44,  3.62s/it, loss=-36.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 47/1400 [06:21<1:18:35,  3.49s/it, loss=-37.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 48/1400 [06:26<1:23:11,  3.69s/it, loss=-39.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 49/1400 [06:29<1:21:52,  3.64s/it, loss=-40.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 50/1400 [06:32<1:19:11,  3.52s/it, loss=-47]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 51/1400 [06:36<1:18:39,  3.50s/it, loss=-47.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 52/1400 [06:39<1:20:00,  3.56s/it, loss=-47.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 53/1400 [06:43<1:20:36,  3.59s/it, loss=-47.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 54/1400 [06:47<1:19:41,  3.55s/it, loss=-48.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 55/1400 [06:51<1:24:45,  3.78s/it, loss=-49]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 56/1400 [06:55<1:29:48,  4.01s/it, loss=-49.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 57/1400 [06:59<1:26:26,  3.86s/it, loss=-50.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 58/1400 [07:03<1:24:21,  3.77s/it, loss=-52.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 59/1400 [07:18<2:40:47,  7.19s/it, loss=-53.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 60/1400 [07:20<2:10:07,  5.83s/it, loss=-54.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 61/1400 [07:23<1:49:59,  4.93s/it, loss=-54.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 62/1400 [07:26<1:36:56,  4.35s/it, loss=-54.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 63/1400 [07:29<1:26:18,  3.87s/it, loss=-55.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 64/1400 [07:32<1:18:04,  3.51s/it, loss=-56.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 65/1400 [07:36<1:21:14,  3.65s/it, loss=-56.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 66/1400 [07:39<1:16:40,  3.45s/it, loss=-56.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 67/1400 [07:42<1:16:11,  3.43s/it, loss=-56.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 68/1400 [07:45<1:15:34,  3.40s/it, loss=-56.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 69/1400 [07:49<1:16:58,  3.47s/it, loss=-57.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 70/1400 [07:52<1:11:58,  3.25s/it, loss=-58]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([5, 1, 500, 500])\n",
      "Labels shape: torch.Size([5, 15])\n",
      "Outputs shape: torch.Size([5, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ICNTrainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     21\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# # Define config dictionary with hyperparameters and metadata\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# config = {\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     \"learning_rate\": 0.001,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#     \"augmentation\": \"No\",\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\master\\InfernoCalibNet\\src\\ICNTrainer.py:111\u001b[0m, in \u001b[0;36mICNTrainer.fit\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Validate for one epoch\u001b[39;00m\n\u001b[0;32m    114\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_one_epoch()\n",
      "File \u001b[1;32m~\\Desktop\\master\\InfernoCalibNet\\src\\ICNTrainer.py:64\u001b[0m, in \u001b[0;36mICNTrainer.train_one_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 64\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Update the TQDM progress bar\u001b[39;00m\n\u001b[0;32m     67\u001b[0m train_progress\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mrunning_loss \u001b[38;5;241m/\u001b[39m (train_progress\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load EfficientNet B3 model\n",
    "model = models.efficientnet_b3(pretrained=True)\n",
    "\n",
    "# Modify the input layer to accept 1-channel grayscale images (instead of the default 3-channel RGB)\n",
    "model.features[0][0] = nn.Conv2d(1, model.features[0][0].out_channels, \n",
    "                                kernel_size=model.features[0][0].kernel_size, \n",
    "                                stride=model.features[0][0].stride, \n",
    "                                padding=model.features[0][0].padding, \n",
    "                                bias=False)\n",
    "\n",
    "# Modify the output layer (classifier) to have 15 classes instead of 1000 (default for ImageNet)\n",
    "num_classes = 15\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move the model to the device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = ICNTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    project_name=\"inferno-calib-net\",\n",
    "    config={\"learning_rate\": 0.001, \"batch_size\": 16, \"epochs\": 5}\n",
    ")\n",
    "\n",
    "trainer.fit(epochs=5)\n",
    "\n",
    "\n",
    "# # Define config dictionary with hyperparameters and metadata\n",
    "# config = {\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"batch_size\": 7,\n",
    "#     \"architecture\": \"EfficientNetB3\",\n",
    "#     \"input_size\": (1000, 1000),\n",
    "#     \"epochs\": 5,\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"loss_function\": \"Weighted BCE\",\n",
    "#     \"class_weights\": \"Computed\",\n",
    "#     \"dataset\": \"X-Ray Images\",\n",
    "#     \"augmentation\": \"No\",\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
